# Multi-Model + Telegram Bot Integration Analysis

**Date:** 2025-12-08  
**Context:** Review existing H3 Telegram Bot before Phase 2.6 implementation

---

## ğŸ” FINDINGS

### âœ… H3 Telegram Approval Bot - EXISTS AND IS GOOD!

**Location:** `services/approval-bot/`

**What It Does:**
- âœ… Async approval workflow (Change Requests via Telegram)
- âœ… File-based system (watches `truth-layer/drift/approvals/pending/`)
- âœ… FastAPI backend (356 lines, clean code)
- âœ… SQLite queue management
- âœ… Inline buttons (Approve/Reject)
- âœ… Already tested and working

**Architecture:**
```
Reconciler â†’ CR YAML â†’ Backend (watchdog) â†’ Telegram notification
    â†“
User clicks âœ…/âŒ
    â†“
Backend writes approval file â†’ Executor applies â†’ Git commit
```

**Status:** âœ… PRODUCTION READY (from H3 phase)

---

## ğŸ’¡ DECISION: Reuse H3 Bot + Extend for Multi-Model

**Why NOT start from scratch:**
1. âœ… Code quality is good (clean, async, tested)
2. âœ… Architecture fits Multi-Model needs perfectly
3. âœ… Already integrated with `.env` (TELEGRAM_BOT_TOKEN, TELEGRAM_CHAT_ID)
4. âœ… File-based workflow = easy to extend
5. âœ… No "legacy debt" - this was built recently with best practices

**What We'll Add to H3 Bot:**
- `/status` - Show system status (LiteLLM, n8n, models health)
- `/costs` - Daily/monthly API spending per model
- `/switch` - Manually switch model priority
- `/logs` - Recent LLM calls from Langfuse
- Multi-model Change Requests (when Observer detects multi-model conflicts)

**What Stays the Same:**
- âœ… Core approval workflow (CR â†’ Telegram â†’ approve â†’ execute)
- âœ… Database structure
- âœ… File watching mechanism
- âœ… Authentication (CHAT_ID whitelist)

---

## ğŸ“‹ UPDATED PLAN: H2 Phase 2.6 with H3 Bot Integration

### Modified Slice 6: Telegram Bot Extension (was: "End-to-End Test")

**Old Goal:** Test Telegram â†’ n8n â†’ LiteLLM  
**New Goal:** Extend H3 bot with multi-model commands + keep approval workflow

**Changes to services/approval-bot/backend.py:**

1. Add command handlers:
   ```python
   @app.command("status")
   async def status_command(update, context):
       # Check LiteLLM health, n8n, models
       # Return: "âœ… GPT-4: OK, Claude: OK, Gemini: OK"
   
   @app.command("costs")
   async def costs_command(update, context):
       # Query Langfuse for today's spending
       # Return: "GPT-4: $2.50, Claude: $1.20, Gemini: $0.05"
   ```

2. Extend CR types to include multi-model conflicts:
   ```python
   # New CR type: MULTI_MODEL_CONFLICT
   # When GPT and Claude both modify same resource
   ```

**Duration:** 90 min â†’ 120 min (added 30 min for new commands)

---

## ğŸ”§ REQUIRED UPDATES (Critical!)

### 1. Model Versions (End of 2025)

**WRONG (in current plan):**
- âŒ Claude 3.5 Sonnet
- âŒ GPT-4 Turbo
- âŒ Gemini 2.0 Flash

**CORRECT (December 2025):**
- âœ… **Claude 4.5 Sonnet** (`anthropic/claude-sonnet-4-5-20250929`)
- âœ… **GPT-5.1** (`openai/gpt-5.1`)
- âœ… **Gemini 3 Pro** (`gemini/gemini-3-pro`)

**Files to Update:**
- `memory-bank/plans/H2_PHASE_2.6_MULTI_MODEL_PLAN.md` (all model references)
- `litellm-config.yaml` (when we create it)
- `memory-bank/docs/CREDENTIALS_REFERENCE.md` (documentation)

---

## ğŸ¯ INTEGRATION STRATEGY

### Phase 2.6 Slice Order (Modified)

**Slices 1-5:** As planned (LiteLLM setup, n8n routing, fallbacks)  
**Slice 6 (NEW):** Extend H3 bot for multi-model  
**Slices 7-12:** As planned (Event Sourcing, production hardening)

---

## ğŸ“Š H3 Bot Current State

**Pros:**
- âœ… Clean codebase
- âœ… Production-tested
- âœ… Async architecture (aiogram)
- âœ… File-based workflow (easy to extend)
- âœ… SQLite state management
- âœ… Inline keyboard UX

**Cons (Minor):**
- âš ï¸ No command handlers yet (only approval callbacks)
- âš ï¸ Not deployed to VPS (still local, but that's H4)
- âš ï¸ No cost tracking integration

**Risk Assessment:** âœ… LOW RISK to reuse and extend

---

## ğŸš€ RECOMMENDATION

**DO NOT start from scratch.**  
**EXTEND H3 bot with multi-model features.**

**Rationale:**
1. Saves 3-4 hours of bot setup work
2. Maintains code quality (H3 bot is well-architected)
3. Unified approval system (one bot for all HITL needs)
4. Natural evolution (Phase 2.6 builds on H3, not replaces it)

**Updated Timeline:**
- Slice 6: 90 min â†’ 120 min (added multi-model commands)
- Total: 10.5 hours â†’ 11 hours (30 min increase)

---

## âœ… ACTION ITEMS

**Before starting Slice 1:**
1. âœ… Update H2_PHASE_2.6_MULTI_MODEL_PLAN.md (model versions)
2. âœ… Update Slice 6 description (Telegram bot extension)
3. âœ… Document H3 bot structure (for future reference)
4. âœ… Verify H3 bot still runs locally (quick test)

**During Slice 6:**
1. Add `/status` command (LiteLLM + models health)
2. Add `/costs` command (Langfuse query)
3. Extend CR types (MULTI_MODEL_CONFLICT)
4. Test: Send command â†’ bot responds

---

## ğŸ“ NOTES

- H3 bot token: `8119131809:AAH...` (in vps.env)
- Chat ID: `5786217215` (Or's Telegram)
- Database: `services/approval-bot/approvals.db` (SQLite)
- Backend: `services/approval-bot/backend.py` (356 lines)

**Last H3 Commit:** e0f8f17 on feature/h2-memory-bank-api  
**Last H3 Test:** âœ… PASSED (manual CR â†’ Telegram â†’ approval â†’ DB update)

---

**Conclusion:** H3 bot is a solid foundation. Extend it, don't replace it.