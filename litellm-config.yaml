# ============================================
# LiteLLM Configuration - Multi-Model Router
# ============================================
# Enables unified API for OpenAI, Claude, Gemini
# Cost tracking + fallback routing
# ============================================

model_list:
  # ==========================================
  # OpenAI Models
  # ==========================================
  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY
      
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY
      
  - model_name: o1-mini
    litellm_params:
      model: openai/o1-mini
      api_key: os.environ/OPENAI_API_KEY
  
  - model_name: o1
    litellm_params:
      model: openai/o1
      api_key: os.environ/OPENAI_API_KEY

  # ==========================================
  # Anthropic (Claude) Models
  # ==========================================
  - model_name: claude-sonnet-4-5
    litellm_params:
      model: anthropic/claude-sonnet-4-5-20250514
      api_key: os.environ/ANTHROPIC_API_KEY
      
  - model_name: claude-sonnet-4
    litellm_params:
      model: anthropic/claude-sonnet-4-20250514
      api_key: os.environ/ANTHROPIC_API_KEY

  - model_name: claude-opus-4
    litellm_params:
      model: anthropic/claude-opus-4-20250514
      api_key: os.environ/ANTHROPIC_API_KEY

# ==========================================
# Router Settings
# ==========================================
router_settings:
  routing_strategy: simple-shuffle  # or: least-busy, latency-based
  model_group_alias:
    fast: gpt-4o-mini
    smart: claude-sonnet-4-5
    reasoning: o1
  fallbacks:
    - gpt-4o: [claude-sonnet-4-5]
    - claude-sonnet-4-5: [gpt-4o]
  
# ==========================================
# Caching & Performance
# ==========================================
litellm_settings:
  drop_params: true  # Drop unsupported params
  success_callback: ["langfuse"]  # Send traces to Langfuse
  cache: false  # Redis cache (disabled for now)
  num_retries: 2
  request_timeout: 600

# ==========================================
# Langfuse Integration (Observability)
# ==========================================
environment_variables:
  LANGFUSE_PUBLIC_KEY: os.environ/LANGFUSE_PUBLIC_KEY
  LANGFUSE_SECRET_KEY: os.environ/LANGFUSE_SECRET_KEY
  LANGFUSE_HOST: os.environ/LANGFUSE_HOST

# ==========================================
# General Settings
# ==========================================
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL
  store_model_in_db: true

# ==========================================
# Notes:
# ==========================================
# 1. All API keys loaded from environment variables
# 2. Langfuse Cloud receives all traces automatically
# 3. Access via: https://api.{DOMAIN}/chat/completions
# 4. Authentication: Header 'Authorization: Bearer {LITELLM_MASTER_KEY}'
# 5. Model routing: Use model_group_alias (fast/smart/reasoning)
# ==========================================
